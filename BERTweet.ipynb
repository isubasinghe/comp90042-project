{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoModel, AutoTokenizer, TFAutoModel, pipeline, AdamW, BertConfig, BertModel, get_linear_schedule_with_warmup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from emoji import demojize\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train.data.jsonl\", \"r\") as f:\n",
    "    raw_lines_train = f.readlines()\n",
    "with open(\"./train.label.json\", \"r\") as f:\n",
    "    raw_labels_train = f.readlines()\n",
    "\n",
    "with open(\"./dev.data.jsonl\", \"r\") as f:\n",
    "    raw_lines_dev = f.readlines()\n",
    "\n",
    "with open(\"./dev.label.json\", \"r\") as f:\n",
    "    raw_labels_dev = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_lines_train = [json.loads(line) for line in raw_lines_train]\n",
    "json_labels_train = [json.loads(line) for line in raw_labels_train][0]\n",
    "\n",
    "json_lines_dev = [json.loads(line) for line in raw_lines_dev]\n",
    "json_labels_dev = [json.loads(line) for line in raw_labels_dev][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(json_lines, json_labels):\n",
    "    X = []\n",
    "    Y = np.zeros(len(json_lines))\n",
    "    \n",
    "    for i, lines in enumerate(json_lines):\n",
    "        X.append(list(map(lambda x: \"<s>\" + x['text'].replace('.', '') + \"</s>\", lines)))\n",
    "        Y[i] = 1 if json_labels[lines[0]['id_str']] == 'rumour' else 0\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>', '<mask>']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = getText(json_lines_train, json_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev, Ydev = getText(json_lines_dev, json_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'How',\n",
       " 'to',\n",
       " 'respond',\n",
       " 'to',\n",
       " 'the',\n",
       " 'murderous',\n",
       " 'attack',\n",
       " 'on',\n",
       " 'Charlie',\n",
       " 'Heb@@',\n",
       " 'do',\n",
       " '?',\n",
       " 'Every',\n",
       " 'newspaper',\n",
       " 'in',\n",
       " 'the',\n",
       " 'free',\n",
       " 'world',\n",
       " 'should',\n",
       " 'print',\n",
       " 'this',\n",
       " 'HTTPURL',\n",
       " '</s>']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(Xtrain[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Isitha\n",
      "[nltk_data]     Subasinghe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Isitha\n",
      "[nltk_data]     Subasinghe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "def build_corpus_and_X(X, vectorizer=None):\n",
    "  corpus = []\n",
    "  newX = []\n",
    "  lem = WordNetLemmatizer()\n",
    "  tokenizer = TweetTokenizer(preserve_case=True, strip_handles=False, reduce_len=True)\n",
    "\n",
    "  for XArray in tqdm(X):\n",
    "    newArray = []\n",
    "    for sent in XArray:\n",
    "      newSent = []\n",
    "      for tok in tokenizer.tokenize(sent.lower()):\n",
    "        if tok not in stopwords.words('english') and tok != '.':\n",
    "          newSent.append(tok)\n",
    "      newSent = ' '.join(newSent)\n",
    "      corpus.append(newSent)\n",
    "      newArray.append(newSent)\n",
    "    newX.append(newArray)\n",
    "  if vectorizer is not None:\n",
    "    vect = vectorizer.transform(corpus)\n",
    "  else:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "  return vectorizer, vect, newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4641/4641 [03:38<00:00, 21.26it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorizer, vect, newX = build_corpus_and_X(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> respond murderous attack charlie hebdo ? every newspaper free world print http://tco/sc2ot63f6j </s>',\n",
       " '<s> @heresy_corner @krustyallslopp jews label anyone like anti-semite campaign person / company finished </s>',\n",
       " '<s> @heresy_corner @krustyallslopp one </s>',\n",
       " '<s> @heresy_corner #imcharliehebdo </s>',\n",
       " '<s> @krustyallslopp ditto </s>',\n",
       " '<s> @grizzly_stats @tom_wein innocent muslims ought find insulting atrocity committed name , sodding cartoon </s>',\n",
       " '<s> @heresy_corner @krustyallslopp yes , becomes </s>',\n",
       " '<s> @heresy_corner @krustyallslopp insult people nothing ? people genuinely offended drawings </s>',\n",
       " '<s> @krustyallslopp @heresy_corner neither ! think little actual muslims </s>',\n",
       " '<s> @berg_han ah , like jews bye bye @krustyallslopp </s>',\n",
       " '<s> @heresy_corner also kid along benign stuff wham like river shite ! </s>',\n",
       " '<s> @berg_han @heresy_corner good point </s>',\n",
       " '<s> @heresy_corner @pjfny ? http://tco/d2qcavkf2h </s>',\n",
       " \"<s> @heresy_corner @krustyallslopp organised jewry , mean , actual people otherwise i'd hating ancestors </s>\",\n",
       " '<s> @theedwardian81 @heresy_corner : http://tco/lmyxpmzw3v </s>',\n",
       " '<s> @heresy_corner @berg_han explored </s>',\n",
       " \"<s> @berg_han @krustyallslopp that's case , problem </s>\",\n",
       " \"<s> @heresy_corner @tom_wein point insulting billions innocent muslims thumb noses bunch lunaticsthey're worth </s>\",\n",
       " '<s> @heresy_corner oh dear saw tweets blocked </s>',\n",
       " \"<s> @berg_han @krustyallslopp learn offended , that's </s>\",\n",
       " \"<s> @heresy_corner @berg_han token jews , blacks irish people would ' learn ' offended either </s>\",\n",
       " '<s> @heresy_corner @berg_han get defend right free speech however much broader context </s>',\n",
       " '<s> @heresy_corner @berg_han record way , shape form defending atrocity </s>',\n",
       " '<s> @krustyallslopp @berg_han yes , remind last time jews bombed guardian </s>',\n",
       " '<s> @heresy_corner know ! gives creeps </s>',\n",
       " \"<s> @heresy_corner there's lot dodgy twitter accounts seem benign see real side :( </s>\",\n",
       " \"<s> @heresy_corner @krustyallslopp people insult something that's important , feel identity attack </s>\",\n",
       " '<s> @krustyallslopp remarkable quickly come woodwork </s>',\n",
       " '<s> @heresy_corner @edzardernst correct response brutality offend lots people * * support brutality ? </s>']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "  def __init__(self, texts, targets, tokenizer, max_len):\n",
    "    self.texts = texts\n",
    "    self.targets = targets\n",
    "    self.tokenzer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = self.texts[item]\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        review,\n",
    "        max_length=self.max_len,\n",
    "        add_special_tokens=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'][0],\n",
    "        'attention_mask': encoding['attention_mask'][0],\n",
    "        'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X, Y, tokenizer, max_len, batch_size):\n",
    "  ds = TweetDataset(\n",
    "      texts = X,\n",
    "      targets = Y,\n",
    "      tokenizer = tokenizer,\n",
    "      max_len = max_len\n",
    "  )\n",
    "  return DataLoader(ds, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXtrain = [ (' '.join(i))[:500] for i in newX ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> respond murderous attack charlie hebdo ? every newspaper free world print http://tco/sc2ot63f6j </s> <s> @heresy_corner @krustyallslopp jews label anyone like anti-semite campaign person / company finished </s> <s> @heresy_corner @krustyallslopp one </s> <s> @heresy_corner #imcharliehebdo </s> <s> @krustyallslopp ditto </s> <s> @grizzly_stats @tom_wein innocent muslims ought find insulting atrocity committed name , sodding cartoon </s> <s> @heresy_corner @krustyallslopp yes , becomes </s> <s'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newXtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 580/580 [00:28<00:00, 20.48it/s]\n"
     ]
    }
   ],
   "source": [
    "_, vect, newXdev = build_corpus_and_X(Xdev, vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXdev = [ (' '.join(i))[:500] for i in newXdev ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_data_loader(newXtrain, Ytrain, tokenizer, 512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = create_data_loader(newXdev, Ydev, tokenizer, 512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumourClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(RumourClassifier, self).__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask\n",
    "    ).pooler_output\n",
    "\n",
    "    output = self.drop(output)\n",
    "    output = self.out(output)\n",
    "    return self.softmax(output)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RumourClassifier(2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_ds) * 150\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, \n",
    "    dl, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_preds = 0\n",
    "\n",
    "  for d in dl:\n",
    "    input_ids = d['input_ids'].to(device)\n",
    "    attention_mask = d['attention_mask'].to(device)\n",
    "    targets = d['targets'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "\n",
    "    loss = loss_fn(output, targets)\n",
    "\n",
    "    correct_preds += torch.sum(pred == targets)\n",
    "\n",
    "    del pred\n",
    "    del targets\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    del output\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping hack\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "  return correct_preds.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_preds = 0 \n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d['input_ids'].to(device)\n",
    "      attention_mask = d['attention_mask'].to(device)\n",
    "      targets = d['targets'].to(device)\n",
    "\n",
    "      output = model(input_ids, attention_mask)\n",
    "\n",
    "      _, pred = torch.max(output, dim=1)\n",
    "\n",
    "      loss = loss_fn(output, targets)\n",
    "\n",
    "      correct_preds += torch.sum(pred == targets)\n",
    "\n",
    "      losses.append(loss.item())\n",
    "\n",
    "      del pred\n",
    "      del targets\n",
    "      del input_ids\n",
    "      del attention_mask\n",
    "      del output\n",
    "\n",
    "  return correct_preds.double() / n_examples, np.mean(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/150 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "<ipython-input-134-94a524199502>:26: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'targets': torch.tensor(self.targets[item], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in tqdm(range(150)):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{150}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_ds,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    len(Ytrain) \n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    test_ds,\n",
    "    loss_fn,\n",
    "    len(Ydev) \n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_bertweet_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
