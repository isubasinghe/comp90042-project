{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoModel, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train.data.jsonl\", \"r\") as f:\n",
    "    raw_lines_train = f.readlines()\n",
    "with open(\"./train.label.json\", \"r\") as f:\n",
    "    raw_labels_train = f.readlines()\n",
    "\n",
    "with open(\"./dev.data.jsonl\", \"r\") as f:\n",
    "    raw_lines_dev = f.readlines()\n",
    "\n",
    "with open(\"./dev.label.json\", \"r\") as f:\n",
    "    raw_labels_dev = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lines_train = [json.loads(line) for line in raw_lines_train]\n",
    "json_labels_train = [json.loads(line) for line in raw_labels_train][0]\n",
    "\n",
    "json_lines_dev = [json.loads(line) for line in raw_lines_dev]\n",
    "json_labels_dev = [json.loads(line) for line in raw_labels_dev][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(json_lines, json_labels):\n",
    "    X = []\n",
    "    Y = np.zeros(len(json_lines))\n",
    "    \n",
    "    for i, lines in enumerate(json_lines):\n",
    "        X.append(list(map(lambda x: x['text'], lines)))\n",
    "        Y[i] = 1 if json_labels[lines[0]['id_str']] == 'rumour' else 0\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = getText(json_lines_train, json_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev, Ydev = getText(json_lines_dev, json_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j',\n",
       " \"@Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished.\",\n",
       " '@Heresy_Corner @KrustyAllslopp \\nNo one does.',\n",
       " '@Heresy_Corner #ImCharlieHebdo',\n",
       " '@KrustyAllslopp Ditto',\n",
       " '@Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon.',\n",
       " '@Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours.',\n",
       " '@Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings.',\n",
       " '@KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims.',\n",
       " \"@berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp\",\n",
       " \"@Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite!\",\n",
       " \"@berg_han @Heresy_Corner It's a good point\",\n",
       " '@Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h',\n",
       " \"@Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors.\",\n",
       " '@theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v',\n",
       " '@Heresy_Corner @berg_han explored.',\n",
       " \"@berg_han @KrustyAllslopp And if that's the case, that is your problem.\",\n",
       " \"@Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it\",\n",
       " '@Heresy_Corner Oh dear... Just saw those tweets... Blocked him.',\n",
       " \"@berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why.\",\n",
       " \"@Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either\",\n",
       " '@Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not',\n",
       " '@Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity.',\n",
       " '@KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian.',\n",
       " '@Heresy_Corner I know!  Gives me the creeps.',\n",
       " \"@Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :(\",\n",
       " \"@Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack.\",\n",
       " \"@KrustyAllslopp It's remarkable how quickly they come out the woodwork.\",\n",
       " \"@Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v, ord=1)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return v/norm\n",
    "\n",
    "def normalised_child_score(data, accessor, modifier= lambda x: x, normalise=True, max_len=40):\n",
    "    scores = np.zeros(max_len)\n",
    "    i = 0\n",
    "    \n",
    "    for entry in data[1:max_len]:\n",
    "        score = modifier(accessor(entry))\n",
    "        scores[i] = score\n",
    "        i += 1\n",
    "    \n",
    "    while i < max_len:\n",
    "        scores[i] = 0\n",
    "        i += 1\n",
    "    \n",
    "    if normalise==False:\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    return normalize(scores)\n",
    "    \n",
    "def get_feature(d, i, tweet_set):\n",
    "    \n",
    "    vec = np.zeros(7 + 40)\n",
    "    i = 0\n",
    "    x = []\n",
    "    \n",
    "    x.append(d['user'][i])\n",
    "    x.append(d['fc'][0][i][0])\n",
    "    x.append(d['rtc'][0][i][0])\n",
    "    x.append(d['verified'][i])\n",
    "    x.append(d['flc'][0][i][0])\n",
    "    x.append(d['lc'][0][i][0])\n",
    "    x.append(d['frc'][0][i][0])\n",
    "    x = x + list(normalised_child_score(tweet_set, lambda x: x['user']['followers_count']))\n",
    "    x = x + list(normalised_child_score(tweet_set, lambda x: x['user']['verified'], normalise=False))\n",
    "    x = x + list(normalised_child_score(tweet_set, lambda x: x['favorite_count']))\n",
    "    x = x + list(normalised_child_score(tweet_set, lambda x: x['retweet_count']))\n",
    "    x = x + list(normalised_child_score(tweet_set, lambda x: x['user']['followers_count']))\n",
    "    x = x + list(normalised_child_score(tweet_set, lambda x: x['user']['friends_count']))\n",
    "    x = np.asarray(x)\n",
    "    return x\n",
    "    \n",
    "def get_features(json_lines, json_labels):\n",
    "    X = np.zeros((len(json_lines),247))\n",
    "    Y = np.zeros(len(json_lines))\n",
    "    \n",
    "    accum = defaultdict(list)\n",
    "    \n",
    "    for i, tweet_set in enumerate(json_lines):\n",
    "        accum['user'].append(int(tweet_set[0]['user']['id']))\n",
    "        accum['favourite_count'].append(tweet_set[0]['favorite_count'])\n",
    "        accum['retweet_count'].append(tweet_set[0]['retweet_count'])\n",
    "        accum['verified'].append(tweet_set[0]['user']['verified'])\n",
    "        accum['followers_count'].append(tweet_set[0]['user']['followers_count'])\n",
    "        accum['listed_count'].append(tweet_set[0]['user']['listed_count'])\n",
    "        accum['friends_count'].append(tweet_set[0]['user']['friends_count'])\n",
    "    \n",
    "    norm = {}\n",
    "    \n",
    "    norm['user']  = accum['user']\n",
    "    \n",
    "    fcscaler = MinMaxScaler()\n",
    "    fcscaler.fit(np.asarray(accum['favourite_count']).reshape(-1, 1))\n",
    "    norm['fc'] = (fcscaler.transform(np.asarray(accum['favourite_count']).reshape(-1, 1)), fcscaler)\n",
    "    \n",
    "    rtscaler = MinMaxScaler()\n",
    "    rtscaler.fit(np.asarray(accum['retweet_count']).reshape(-1, 1))\n",
    "    norm['rtc'] = (rtscaler.transform(np.asarray(accum['retweet_count']).reshape(-1, 1)), rtscaler)\n",
    "    \n",
    "    norm['verified'] = np.asarray(list(map(lambda x: int(x), accum['verified'])))\n",
    "    \n",
    "    flcscaler = MinMaxScaler()\n",
    "    flcscaler.fit(np.asarray(accum['followers_count']).reshape(-1, 1))\n",
    "    norm['flc'] = (flcscaler.transform(np.asarray(accum['followers_count']).reshape(-1, 1)), flcscaler)\n",
    "    \n",
    "    lcscaler = MinMaxScaler()\n",
    "    lcscaler.fit(np.asarray(accum['listed_count']).reshape(-1, 1))\n",
    "    norm['lc'] = (lcscaler.transform(np.asarray(accum['listed_count']).reshape(-1, 1)), lcscaler)\n",
    "    \n",
    "    frcscaler = MinMaxScaler()\n",
    "    frcscaler.fit(np.asarray(accum['friends_count']).reshape(-1, 1))\n",
    "    norm['frc'] = (frcscaler.transform(np.asarray(accum['friends_count']).reshape(-1, 1)), frcscaler)\n",
    "    \n",
    "    for i, tweet_set in enumerate(json_lines):\n",
    "        X[i, :] = (get_feature(norm, i, tweet_set))\n",
    "        Y[i] = 1 if json_labels[tweet_set[0]['id_str']] == 'rumour' else 0\n",
    "\n",
    "    return X, Y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = get_features(json_lines_train, json_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev, Ydev = get_features(json_lines_dev, json_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, others, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenzer = tokenizer\n",
    "        self.others = others\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        review = ' '.join(tokenizer.tokenize(self.texts[item])[:470])\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0],\n",
    "            'others': torch.tensor(self.others[item], dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X, others, Y, tokenizer, max_len, batch_size):\n",
    "    ds = TweetDataset(\n",
    "        texts = X,\n",
    "        others = others,\n",
    "        targets = Y,\n",
    "        tokenizer = tokenizer,\n",
    "        max_len = max_len\n",
    "      )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(json_lines, json_labels):\n",
    "    X = []\n",
    "    Y = np.zeros(len(json_lines))\n",
    "    \n",
    "    for i, lines in enumerate(json_lines):\n",
    "        X.append(list(map(lambda x: x['text'], lines)))\n",
    "        Y[i] = 1 if json_labels[lines[0]['id_str']] == 'rumour' else 0\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nXtrain, nYtrain = getText(json_lines_train, json_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nXdev, nYdev = getText(json_lines_dev, json_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Isitha\n",
      "[nltk_data]     Subasinghe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Isitha\n",
      "[nltk_data]     Subasinghe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "def build_corpus_and_X(X, vectorizer=None):\n",
    "  corpus = []\n",
    "  newX = []\n",
    "  lem = WordNetLemmatizer()\n",
    "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=False, reduce_len=True)\n",
    "\n",
    "  for XArray in tqdm(X):\n",
    "    newArray = []\n",
    "    for sent in XArray:\n",
    "      newSent = []\n",
    "      for tok in tokenizer.tokenize(sent.lower()):\n",
    "        if tok not in stopwords.words('english') and tok != '.':\n",
    "          newSent.append(tok)\n",
    "      newSent = ' '.join(newSent)\n",
    "      corpus.append(newSent)\n",
    "      newArray.append(newSent)\n",
    "    newX.append(newArray)\n",
    "  if vectorizer is not None:\n",
    "    vect = vectorizer.transform(corpus)\n",
    "  else:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "  return vectorizer, vect, newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4641/4641 [03:41<00:00, 20.92it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorizer, vect, newX = build_corpus_and_X(nXtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 580/580 [00:28<00:00, 20.14it/s]\n"
     ]
    }
   ],
   "source": [
    "_, vect, newXdev = build_corpus_and_X(nXdev, vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumourClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(RumourClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask\n",
    "    ).pooler_output\n",
    "\n",
    "    output = self.drop(output)\n",
    "    output = self.out(output)\n",
    "    return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX_ = [' '.join(a) for a in newX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXdev_ = [' '.join(a) for a in newXdev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"respond murderous attack charlie hebdo ? every newspaper free world print http://t.co/sc2ot63f6j @heresy_corner @krustyallslopp jews label anyone like anti-semite campaign person / company finished @heresy_corner @krustyallslopp one @heresy_corner #imcharliehebdo @krustyallslopp ditto @grizzly_stats @tom_wein innocent muslims ought find insulting atrocity committed name , sodding cartoon @heresy_corner @krustyallslopp yes , becomes @heresy_corner @krustyallslopp insult people nothing ? people genuinely offended drawings @krustyallslopp @heresy_corner neither ! think little actual muslims @berg_han ah , like jews bye bye @krustyallslopp @heresy_corner also kid along benign stuff ... wham like river shite ! @berg_han @heresy_corner good point @heresy_corner @pjfny ? http://t.co/d2qcavkf2h @heresy_corner @krustyallslopp organised jewry , mean , actual people otherwise i'd hating ancestors @theedwardian81 @heresy_corner ... : http://t.co/lmyxpmzw3v @heresy_corner @berg_han explored @berg_han @krustyallslopp that's case , problem @heresy_corner @tom_wein point insulting billions innocent muslims thumb noses bunch lunatics.they ' worth @heresy_corner oh dear ... saw tweets ... blocked @berg_han @krustyallslopp learn offended , that's @heresy_corner @berg_han token jews , blacks irish people would ' learn ' offended either @heresy_corner @berg_han get ... defend right free speech however much broader context @heresy_corner @berg_han record way , shape form defending atrocity @krustyallslopp @berg_han yes , remind last time jews bombed guardian @heresy_corner know ! gives creeps @heresy_corner there's lot dodgy twitter accounts seem benign see real side :( @heresy_corner @krustyallslopp people insult something that's important , feel identity attack @krustyallslopp remarkable quickly come woodwork @heresy_corner @edzardernst correct response brutality offend lots people * * support brutality ?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newX_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_data_loader(newX_, Xtrain, Ytrain, tokenizer, 512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = create_data_loader(newXdev_, Xdev, Ydev, tokenizer, 512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81923fbb43c840b5ac484ba98f1eacc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RumourClassifier(2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "<ipython-input-173-8d871bcc4461>:28: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'targets': torch.tensor(self.targets[item], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5568, 0.4432],\n",
      "        [0.6460, 0.3540]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "targets = data['targets'].to(device)\n",
    "output = model(input_ids, attention_mask)\n",
    "print(output)\n",
    "print(output.shape)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del input_ids\n",
    "del attention_mask\n",
    "del targets\n",
    "del output\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_ds) * 150\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, \n",
    "    dl, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_preds = 0\n",
    "\n",
    "  for i, d in enumerate(dl):\n",
    "    if i%100 == 0:\n",
    "      print(i)\n",
    "    input_ids = d['input_ids'].to(device)\n",
    "    attention_mask = d['attention_mask'].to(device)\n",
    "    targets = d['targets'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "    loss = loss_fn(output, targets)\n",
    "\n",
    "    correct_preds += torch.sum(pred == targets)\n",
    "\n",
    "    del pred\n",
    "    del targets\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    del output\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping hack\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "  return correct_preds.double() / n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_preds = 0 \n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d['input_ids'].to(device)\n",
    "      attention_mask = d['attention_mask'].to(device)\n",
    "      targets = d['targets'].to(device)\n",
    "\n",
    "      output = model(input_ids, attention_mask)\n",
    "\n",
    "      _, pred = torch.max(output, dim=1)\n",
    "\n",
    "      loss = loss_fn(output, targets)\n",
    "\n",
    "      correct_preds += torch.sum(pred == targets)\n",
    "\n",
    "      losses.append(loss.item())\n",
    "\n",
    "      del pred\n",
    "      del targets\n",
    "      del input_ids\n",
    "      del attention_mask\n",
    "      del output\n",
    "\n",
    "  return correct_preds.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/150 [00:00<?, ?it/s]<ipython-input-15-8d871bcc4461>:28: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'targets': torch.tensor(self.targets[item], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Train loss 0.644572954024866 accuracy 0.6589097177332471\n",
      "Val   loss 0.6286909202049519 accuracy 0.6775862068965517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                             | 1/150 [06:10<15:20:42, 370.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/150\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Train loss 0.6431218982256249 accuracy 0.658694246929541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                             | 2/150 [12:21<15:14:44, 370.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6287589603456958 accuracy 0.6775862068965517\n",
      "\n",
      "Epoch 3/150\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in tqdm(range(150)):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{150}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_ds,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    len(Ytrain) \n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    test_ds,\n",
    "    loss_fn,\n",
    "    len(Ydev) \n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state1.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
