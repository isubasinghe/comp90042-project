{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import BigBirdTokenizer, BigBirdModel, BertTokenizer, AutoModel, AutoTokenizer, TFAutoModel, pipeline, AdamW, BertConfig, BertModel, get_linear_schedule_with_warmup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import argparse\n",
    "import numba\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_format, has_output=True):\n",
    "  raw_lines = None\n",
    "  with open(f\"{file_format}.data.jsonl\", \"r\") as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "  raw_labels = None\n",
    "  if has_output == True:\n",
    "    with open(f\"{file_format}.label.json\", \"r\") as f:\n",
    "      raw_labels = json.loads(f.read())\n",
    "\n",
    "  return ([json.loads(line) for line in raw_lines], raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(tweets, labels):\n",
    "  assert len(tweets) == len(labels)\n",
    "  tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)\n",
    "  X = []\n",
    "  Y = np.zeros(len(labels))\n",
    "  for i, tweet_set in tqdm(enumerate(tweets)):\n",
    "    arr = []\n",
    "    for tweet in tweet_set:\n",
    "      tweet = ' '.join(tokenizer.tokenize(tweet['text']))\n",
    "      arr.append(tweet)\n",
    "    X.append('.'.join(arr))\n",
    "    if labels != None:\n",
    "      Y[i] = 1 if labels[tweet_set[0]['id_str']] == 'rumour' else 0\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "  def __init__(self, texts, targets, tokenizer, max_len):\n",
    "    self.texts = texts\n",
    "    self.targets = targets\n",
    "    self.tokenzer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = self.texts[item]\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        review,\n",
    "        max_length=self.max_len,\n",
    "        add_special_tokens=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'][0],\n",
    "        'attention_mask': encoding['attention_mask'][0],\n",
    "        'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X, Y, tokenizer, max_len, batch_size):\n",
    "  ds = TweetDataset(\n",
    "      texts = X,\n",
    "      targets = Y,\n",
    "      tokenizer = tokenizer,\n",
    "      max_len = max_len\n",
    "  )\n",
    "  return DataLoader(ds, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RumourClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(RumourClassifier, self).__init__()\n",
    "    self.bert = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "    self.cls_layer = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask\n",
    "    ).last_hidden_state\n",
    "    \n",
    "    cls_rep = output[:, 0]\n",
    "    \n",
    "    logits = self.cls_layer(cls_rep)\n",
    "    \n",
    "    \n",
    "    return logits \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4641it [00:07, 612.36it/s]\n",
      "580it [00:00, 606.12it/s]\n"
     ]
    }
   ],
   "source": [
    "train_entries, train_labels = load_data(\"train\")\n",
    "trainX, trainY = get_xy(train_entries, train_labels)\n",
    "\n",
    "dev_entries, dev_labels = load_data(\"dev\")\n",
    "devX, devY = get_xy(dev_entries, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RumourClassifier(2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_data_loader(trainX, trainY, tokenizer, 1400, 1)\n",
    "dev_ds = create_data_loader(devX, devY, tokenizer, 1400, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "<ipython-input-5-94a524199502>:26: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'targets': torch.tensor(self.targets[item], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "total_steps = len(train_ds) * NO_EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps):\n",
    "\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        for it, data in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = data['input_ids'].to(device), data['attention_mask'].to(device), data['targets'].to(device)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "            scheduler.step()\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                \n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "        \n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            seq, attn_masks, labels = data['input_ids'].to(device), data['attention_mask'].to(device),data['targets'].to(device)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "<ipython-input-5-94a524199502>:26: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'targets': torch.tensor(self.targets[item], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.4924807846546173; Accuracy: 1.0; Time taken (s): 1.1453042030334473\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.9756990671157837; Accuracy: 0.0; Time taken (s): 38.87397360801697\n",
      "Iteration 200 of epoch 0 complete. Loss: 0.4740241765975952; Accuracy: 1.0; Time taken (s): 38.74835205078125\n",
      "Iteration 300 of epoch 0 complete. Loss: 1.155455231666565; Accuracy: 0.0; Time taken (s): 38.90450119972229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(model, loss_fn, optimizer, train_ds, dev_ds, NO_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
